#!/usr/bin/env python3
"""
DOI-URL CSV Importer for PostgreSQL
===================================

This script imports the CSV file generated by the OpenAlex URL extractor
into PostgreSQL tables for efficient DOI-URL mapping.

Features:
- Efficient batch processing with configurable batch sizes
- Data validation and cleaning
- Duplicate handling with upsert capability
- Progress tracking and logging
- Resume functionality for interrupted imports with file integrity checking
- URL quality scoring based on source characteristics
- Import history tracking and monitoring
- .env file support for database credentials

Database Configuration:
    The script supports two ways to provide database credentials:

    1. Command line arguments (takes precedence):
       --db-host, --db-port, --db-name, --db-user, --db-password

    2. .env file with the following variables:
       POSTGRES_HOST, POSTGRES_PORT, POSTGRES_DB, POSTGRES_USER, POSTGRES_PASSWORD

    If a .env file is present, database arguments become optional on the command line.
    Command line arguments always override .env file values.

Usage:
    # With command line arguments
    python doi_url_importer.py --csv-file doi_urls.csv --db-name biomedical
                               --db-user myuser --db-password mypass

    # With .env file (database args optional)
    python doi_url_importer.py --csv-file doi_urls.csv

    # Resume interrupted import
    python doi_url_importer.py --csv-file doi_urls.csv --resume
"""

__version__ = 0.3

import argparse
import csv
import hashlib
import logging
import os
import sys
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Generator
from urllib.parse import urlparse
import re

# count_lines_fast is imported dynamically in _count_csv_rows method
# from helpers.csv_utils import count_lines_fast

try:
    from tqdm import tqdm
except ImportError:
    print("Error: tqdm not found. Install with: pip install tqdm")
    sys.exit(1)

try:
    import psycopg2
    from psycopg2.extras import RealDictCursor
except ImportError:
    print("Error: psycopg2 not found. Install with: pip install psycopg2-binary")
    sys.exit(1)

# Try to import python-dotenv for .env file support
try:
    from dotenv import load_dotenv
    DOTENV_AVAILABLE = True
except ImportError:
    DOTENV_AVAILABLE = False

# Import database creation utilities
try:
    from db.create_db import DatabaseCreator
except ImportError:
    # Fallback for when running from different directory
    sys.path.append(str(Path(__file__).parent))
    from db.create_db import DatabaseCreator

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('doi_url_import.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


def load_env_config() -> Optional[Dict[str, str]]:
    """
    Load database configuration from .env file if available.

    Returns:
        Dictionary with database configuration or None if .env not available
    """
    if not DOTENV_AVAILABLE:
        return None

    env_file = Path('.env')
    if not env_file.exists():
        return None

    # Load .env file
    load_dotenv(env_file)

    # Extract database configuration
    config = {}
    env_mappings = {
        'host': 'POSTGRES_HOST',
        'port': 'POSTGRES_PORT',
        'database': 'POSTGRES_DB',
        'user': 'POSTGRES_USER',
        'password': 'POSTGRES_PASSWORD'
    }

    for config_key, env_key in env_mappings.items():
        value = os.getenv(env_key)
        if value:
            config[config_key] = value

    # Convert port to integer if present
    if 'port' in config:
        try:
            config['port'] = int(config['port'])
        except ValueError:
            logger.warning(f"Invalid port value in .env: {config['port']}")
            del config['port']

    return config if config else None


class DOIURLImporter:
    def __init__(self, db_config: Dict[str, Any], csv_file: str,
                 batch_size: int = 10000, create_tables: bool = True, resume: bool = False):
        self.db_config = db_config
        self.csv_file = Path(csv_file)
        self.batch_size = batch_size
        self.create_tables = create_tables
        self.resume = resume

        # Resume state
        self.import_id = None
        self.csv_file_hash = None
        self.start_row = 0  # Row number to start from (0-based, excluding header)
        self.total_csv_rows = 0

        # Lookup table caches for normalized database
        self.lookup_caches = {
            'license': {},
            'oa_status': {},
            'host_type': {},
            'work_type': {}
        }

        # Track if indexes are disabled for bulk import performance
        self.indexes_disabled = False

        # Statistics
        self.stats = {
            'total_rows_processed': 0,
            'rows_inserted': 0,
            'rows_updated': 0,
            'rows_skipped': 0,
            'duplicate_dois': 0,
            'batch_duplicates': 0,  # Duplicates found within batches
            'invalid_urls': 0,
            'start_time': None,
            'end_time': None,
            'resumed_from_row': 0,
            'cache_hits': 0,
            'cache_misses': 0
        }

    def connect_db(self):
        """Establish database connection"""
        try:
            conn = psycopg2.connect(**self.db_config)
            return conn
        except psycopg2.Error as e:
            logger.error(f"Database connection failed: {e}")
            raise

    def get_or_create_lookup_id(self, table_name: str, value: str, connection=None) -> Optional[int]:
        """
        Get or create a lookup table entry and return its ID with caching.

        PERFORMANCE OPTIMIZED: Reuses existing connection to avoid connection overhead.

        Args:
            table_name: Name of the lookup table (license, oa_status, host_type, work_type)
            value: The value to look up or create
            connection: Optional existing database connection to reuse

        Returns:
            The ID of the lookup entry, or None if value is empty
        """
        if not value or not value.strip():
            return None

        value = value.strip()

        # Check cache first
        if value in self.lookup_caches[table_name]:
            self.stats['cache_hits'] += 1
            return self.lookup_caches[table_name][value]

        self.stats['cache_misses'] += 1

        # Use provided connection or create new one
        if connection:
            return self._get_or_create_lookup_with_connection(table_name, value, connection)
        else:
            with self.connect_db() as conn:
                return self._get_or_create_lookup_with_connection(table_name, value, conn)

    def _get_or_create_lookup_with_connection(self, table_name: str, value: str, connection) -> Optional[int]:
        """Helper method to handle lookup operations with an existing connection."""
        with connection.cursor() as cur:
            try:
                # Try to get existing ID
                cur.execute(f"""
                SELECT id FROM unpaywall.{table_name} WHERE value = %s
                """, (value,))

                result = cur.fetchone()
                if result:
                    lookup_id = result[0]
                    self.lookup_caches[table_name][value] = lookup_id
                    return lookup_id

                # Create new entry
                cur.execute(f"""
                INSERT INTO unpaywall.{table_name} (value)
                VALUES (%s) RETURNING id
                """, (value,))

                new_id = cur.fetchone()[0]
                # Note: Don't commit here - let the caller handle transaction management

                # Cache the new ID
                self.lookup_caches[table_name][value] = new_id
                return new_id

            except psycopg2.Error as e:
                logger.error(f"Failed to get/create lookup ID for {table_name}.{value}: {e}")
                return None

    def preload_lookup_caches(self):
        """
        Preload all lookup table caches to improve import performance.
        """
        logger.info("Preloading lookup table caches...")

        with self.connect_db() as conn:
            with conn.cursor() as cur:
                for table_name in self.lookup_caches.keys():
                    try:
                        cur.execute(f"SELECT id, value FROM unpaywall.{table_name}")
                        results = cur.fetchall()

                        for lookup_id, value in results:
                            self.lookup_caches[table_name][value] = lookup_id

                        logger.info(f"Preloaded {len(results)} entries for {table_name}")

                    except psycopg2.Error as e:
                        logger.warning(f"Failed to preload cache for {table_name}: {e}")

        total_cached = sum(len(cache) for cache in self.lookup_caches.values())
        logger.info(f"Total lookup entries cached: {total_cached}")

    def batch_create_lookup_entries(self, lookup_values: Dict[str, set], connection) -> None:
        """
        PERFORMANCE OPTIMIZED: Batch create multiple lookup table entries at once.

        Args:
            lookup_values: Dict mapping table names to sets of values to create
            connection: Database connection to use
        """
        logger.debug("Batch creating lookup table entries...")

        with connection.cursor() as cur:
            for table_name, values in lookup_values.items():
                if not values:
                    continue

                # Filter out values that are already cached
                new_values = [v for v in values if v not in self.lookup_caches[table_name]]

                if not new_values:
                    continue

                logger.debug(f"Batch creating {len(new_values)} entries for {table_name}")

                try:
                    # Use execute_many for bulk insert
                    cur.executemany(f"""
                        INSERT INTO unpaywall.{table_name} (value)
                        VALUES (%s)
                        ON CONFLICT (value) DO NOTHING
                    """, [(value,) for value in new_values])

                    # Fetch the IDs for the new entries
                    if new_values:
                        placeholders = ','.join(['%s'] * len(new_values))
                        cur.execute(f"""
                            SELECT id, value FROM unpaywall.{table_name}
                            WHERE value IN ({placeholders})
                        """, new_values)

                        results = cur.fetchall()
                        for lookup_id, value in results:
                            self.lookup_caches[table_name][value] = lookup_id

                        logger.debug(f"Cached {len(results)} new entries for {table_name}")

                except psycopg2.Error as e:
                    logger.warning(f"Failed to batch create lookup entries for {table_name}: {e}")

            # Commit all lookup table changes
            connection.commit()

    def normalize_location_type(self, location_type: str) -> str:
        """
        Convert location type text to single character format.

        Args:
            location_type: Text location type (primary, alternate, best_oa, etc.)

        Returns:
            Single character: 'p' for primary, 'a' for alternate, 'b' for best_oa
        """
        if not location_type:
            return 'p'  # Default to primary

        location_type = location_type.lower().strip()

        # Map common variations
        if location_type in ['primary', 'p']:
            return 'p'
        elif location_type in ['alternate', 'alternative', 'a']:
            return 'a'
        elif location_type in ['best_oa', 'best', 'b']:
            return 'b'
        else:
            logger.warning(f"Unknown location type '{location_type}', defaulting to 'p'")
            return 'p'

    def disable_indexes_for_bulk_import(self):
        """
        Disable non-essential indexes during bulk import for better performance.
        Keeps primary key and unique constraints but drops other indexes.
        """
        if self.indexes_disabled:
            return

        logger.info("Disabling indexes for bulk import performance...")

        # List of indexes to disable (non-essential ones)
        indexes_to_disable = [
            'idx_unpaywall_doi_urls_doi',
            'idx_unpaywall_doi_urls_url',
            'idx_unpaywall_doi_urls_pdf_url',
            'idx_unpaywall_doi_urls_doi_location_type',
            'idx_unpaywall_doi_urls_license_id',
            'idx_unpaywall_doi_urls_oa_status_id',
            'idx_unpaywall_doi_urls_host_type_id',
            'idx_unpaywall_doi_urls_work_type_id',
            'idx_unpaywall_doi_urls_location_type',
            'idx_unpaywall_doi_urls_publication_year',
            'idx_unpaywall_doi_urls_is_retracted',
            'idx_unpaywall_doi_urls_openalex_work_id'
        ]

        with self.connect_db() as conn:
            with conn.cursor() as cur:
                disabled_count = 0
                for index_name in indexes_to_disable:
                    try:
                        # Check if index exists first
                        cur.execute("""
                        SELECT 1 FROM pg_indexes
                        WHERE schemaname = 'unpaywall'
                        AND tablename = 'doi_urls'
                        AND indexname = %s
                        """, (index_name,))

                        if cur.fetchone():
                            cur.execute(f"DROP INDEX IF EXISTS unpaywall.{index_name}")
                            disabled_count += 1
                            logger.debug(f"Disabled index: {index_name}")

                    except psycopg2.Error as e:
                        logger.warning(f"Failed to disable index {index_name}: {e}")

                conn.commit()
                logger.info(f"Disabled {disabled_count} indexes for bulk import")
                self.indexes_disabled = True

    def recreate_indexes_after_import(self):
        """
        Recreate indexes after bulk import is complete.
        """
        if not self.indexes_disabled:
            return

        logger.info("Recreating indexes after bulk import...")

        # Index definitions to recreate
        index_definitions = [
            ("idx_unpaywall_doi_urls_doi", "CREATE INDEX idx_unpaywall_doi_urls_doi ON unpaywall.doi_urls(doi)"),
            ("idx_unpaywall_doi_urls_url", "CREATE INDEX idx_unpaywall_doi_urls_url ON unpaywall.doi_urls(url)"),
            ("idx_unpaywall_doi_urls_pdf_url", "CREATE INDEX idx_unpaywall_doi_urls_pdf_url ON unpaywall.doi_urls(pdf_url) WHERE pdf_url IS NOT NULL"),
            ("idx_unpaywall_doi_urls_doi_location_type", "CREATE INDEX idx_unpaywall_doi_urls_doi_location_type ON unpaywall.doi_urls(doi, location_type)"),
            ("idx_unpaywall_doi_urls_license_id", "CREATE INDEX idx_unpaywall_doi_urls_license_id ON unpaywall.doi_urls(license_id)"),
            ("idx_unpaywall_doi_urls_oa_status_id", "CREATE INDEX idx_unpaywall_doi_urls_oa_status_id ON unpaywall.doi_urls(oa_status_id)"),
            ("idx_unpaywall_doi_urls_host_type_id", "CREATE INDEX idx_unpaywall_doi_urls_host_type_id ON unpaywall.doi_urls(host_type_id)"),
            ("idx_unpaywall_doi_urls_work_type_id", "CREATE INDEX idx_unpaywall_doi_urls_work_type_id ON unpaywall.doi_urls(work_type_id)"),
            ("idx_unpaywall_doi_urls_location_type", "CREATE INDEX idx_unpaywall_doi_urls_location_type ON unpaywall.doi_urls(location_type)"),
            ("idx_unpaywall_doi_urls_publication_year", "CREATE INDEX idx_unpaywall_doi_urls_publication_year ON unpaywall.doi_urls(publication_year)"),
            ("idx_unpaywall_doi_urls_is_retracted", "CREATE INDEX idx_unpaywall_doi_urls_is_retracted ON unpaywall.doi_urls(is_retracted)"),
            ("idx_unpaywall_doi_urls_openalex_work_id", "CREATE INDEX idx_unpaywall_doi_urls_openalex_work_id ON unpaywall.doi_urls(openalex_id)")
        ]

        with self.connect_db() as conn:
            with conn.cursor() as cur:
                recreated_count = 0
                for index_name, index_sql in index_definitions:
                    try:
                        cur.execute(index_sql)
                        recreated_count += 1
                        logger.debug(f"Recreated index: {index_name}")

                    except psycopg2.Error as e:
                        logger.warning(f"Failed to recreate index {index_name}: {e}")

                conn.commit()
                logger.info(f"Recreated {recreated_count} indexes after bulk import")
                self.indexes_disabled = False

    def _calculate_file_hash(self) -> str:
        """Calculate SHA-256 hash of the CSV file for change detection"""
        hash_sha256 = hashlib.sha256()
        with open(self.csv_file, 'rb') as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_sha256.update(chunk)
        return hash_sha256.hexdigest()

    def _count_csv_rows(self) -> int:
        """Count total rows in CSV file (excluding header) using fast line counting"""
        if not hasattr(self, '_cached_line_count'):
            from helpers.csv_utils import count_lines_fast
            logger.info("Counting CSV lines...")
            self._cached_line_count = count_lines_fast(self.csv_file) - 1  # Subtract 1 for header
            logger.info(f"CSV has {self._cached_line_count:,} data rows")
        return self._cached_line_count

    def _generate_import_id(self) -> str:
        """Generate unique import ID based on file path and timestamp"""
        import uuid
        return f"{self.csv_file.stem}_{int(time.time())}_{str(uuid.uuid4())[:8]}"

    def create_schema(self):
        """Create the DOI-URL mapping tables using the centralized DatabaseCreator"""
        logger.info("Creating DOI-URL schema using DatabaseCreator...")

        try:
            # Create DatabaseCreator instance with the same connection config
            creator = DatabaseCreator(**self.db_config)

            # Create the complete schema without indexes for bulk import performance
            # Indexes will be created after the import is complete
            success = creator.create_complete_schema(verify=True, create_indexes=False)

            if not success:
                raise RuntimeError("Schema creation failed")

            logger.info("Schema creation completed successfully (indexes will be created after import)")

        except Exception as e:
            logger.error(f"Schema creation failed: {e}")
            raise

    def _check_existing_import(self) -> Optional[Dict[str, Any]]:
        """Check if there's an existing incomplete import for this file"""
        with self.connect_db() as conn:
            with conn.cursor(cursor_factory=RealDictCursor) as cur:
                cur.execute("""
                    SELECT * FROM unpaywall.import_progress
                    WHERE csv_file_path = %s AND status = 'in_progress'
                    ORDER BY start_time DESC LIMIT 1
                """, (str(self.csv_file),))
                return cur.fetchone()

    def _create_import_record(self) -> str:
        """Create a new import progress record"""
        import_id = self._generate_import_id()

        with self.connect_db() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    INSERT INTO unpaywall.import_progress
                    (import_id, csv_file_path, csv_file_hash, total_rows, status)
                    VALUES (%s, %s, %s, %s, 'in_progress')
                """, (import_id, str(self.csv_file), self.csv_file_hash, self.total_csv_rows))
                conn.commit()

        logger.info(f"Created import record: {import_id}")
        return import_id

    def _update_import_progress(self, processed_rows: int, batch_id: int):
        """Update import progress"""
        with self.connect_db() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    UPDATE unpaywall.import_progress
                    SET processed_rows = %s, last_batch_id = %s, updated_at = CURRENT_TIMESTAMP
                    WHERE import_id = %s
                """, (processed_rows, batch_id, self.import_id))
                conn.commit()

    def _complete_import(self, success: bool = True, error_message: Optional[str] = None):
        """Mark import as completed or failed"""
        status = 'completed' if success else 'failed'

        with self.connect_db() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    UPDATE unpaywall.import_progress
                    SET status = %s, end_time = CURRENT_TIMESTAMP, error_message = %s, updated_at = CURRENT_TIMESTAMP
                    WHERE import_id = %s
                """, (status, error_message, self.import_id))
                conn.commit()

        logger.info(f"Import {self.import_id} marked as {status}")

    def _setup_resume_state(self):
        """Setup resume state by checking for existing imports"""
        self.csv_file_hash = self._calculate_file_hash()
        self.total_csv_rows = self._count_csv_rows()

        if self.resume:
            existing_import = self._check_existing_import()

            if existing_import:
                # Check if file has changed
                if existing_import['csv_file_hash'] != self.csv_file_hash:
                    logger.warning(f"CSV file has changed since last import (hash mismatch)")
                    logger.warning(f"Previous hash: {existing_import['csv_file_hash']}")
                    logger.warning(f"Current hash: {self.csv_file_hash}")

                    response = input("File has changed. Continue with new import? (y/N): ")
                    if response.lower() != 'y':
                        logger.info("Import cancelled by user")
                        sys.exit(0)

                    # Mark old import as cancelled and start new one
                    with self.connect_db() as conn:
                        with conn.cursor() as cur:
                            cur.execute("""
                                UPDATE unpaywall.import_progress
                                SET status = 'cancelled', end_time = CURRENT_TIMESTAMP,
                                    error_message = 'File changed, new import started'
                                WHERE import_id = %s
                            """, (existing_import['import_id'],))
                            conn.commit()

                    self.import_id = self._create_import_record()
                    self.start_row = 0
                else:
                    # Resume from existing import
                    self.import_id = existing_import['import_id']
                    self.start_row = existing_import['processed_rows']
                    self.stats['resumed_from_row'] = self.start_row

                    logger.info(f"Resuming import {self.import_id} from row {self.start_row}")
                    logger.info(f"Progress: {self.start_row}/{self.total_csv_rows} rows ({self.start_row/self.total_csv_rows*100:.1f}%)")
            else:
                logger.info("No existing incomplete import found, starting new import")
                self.import_id = self._create_import_record()
                self.start_row = 0
        else:
            # Not resuming, start fresh
            self.import_id = self._create_import_record()
            self.start_row = 0

    def validate_and_clean_row(self, row: Dict[str, str], connection=None) -> Optional[Dict[str, Any]]:
        """
        Validate and clean a single CSV row for normalized database structure.

        PERFORMANCE OPTIMIZED: Accepts optional connection to reuse for lookup operations.
        """

        # Required fields
        doi_raw = row.get('doi', '').strip()
        url = row.get('url', '').strip()
        location_type = row.get('location_type', '').strip()

        if not doi_raw or not url or not location_type:
            return None

        # Extract DOI identifier from DOI URL if needed
        doi_identifier = self._extract_doi_identifier(doi_raw)
        if not doi_identifier:
            logger.warning(f"Invalid DOI format: {doi_raw}")
            return None

        # Validate URL
        if not self._is_valid_url(url):
            self.stats['invalid_urls'] += 1
            logger.warning(f"Invalid URL: {url}")
            return None

        # Convert text values to foreign key IDs using cached lookups
        # Pass connection to avoid creating new connections for each lookup
        license_id = self.get_or_create_lookup_id('license', row.get('license', '').strip(), connection)
        oa_status_id = self.get_or_create_lookup_id('oa_status', row.get('oa_status', '').strip(), connection)
        host_type_id = self.get_or_create_lookup_id('host_type', row.get('host_type', '').strip(), connection)
        work_type_id = self.get_or_create_lookup_id('work_type', row.get('work_type', '').strip(), connection)

        # Normalize location type to single character
        normalized_location_type = self.normalize_location_type(location_type)

        # Clean and convert data for normalized structure
        cleaned_row = {
            'doi': doi_identifier.lower(),  # Store just the DOI identifier, normalized
            'url': url,
            'pdf_url': row.get('pdf_url', '').strip() or None,
            'openalex_id': self._safe_openalex_id(row.get('openalex_id', '')),
            'title': row.get('title', '').strip() or None,
            'publication_year': self._safe_int(row.get('publication_year', '')),
            'location_type': normalized_location_type,  # Single character format
            'version': row.get('version', '').strip() or None,
            'license_id': license_id,  # Foreign key ID instead of text
            'host_type_id': host_type_id,  # Foreign key ID instead of text
            'oa_status_id': oa_status_id,  # Foreign key ID instead of text
            'is_oa': self._safe_bool(row.get('is_oa', '')),
            'work_type_id': work_type_id,  # Foreign key ID instead of text
            'is_retracted': self._safe_bool(row.get('is_retracted', '')),
            'url_quality_score': self._calculate_url_quality_score(row)
        }

        return cleaned_row

    def _extract_doi_identifier(self, doi_input: str) -> Optional[str]:
        """Extract DOI identifier from either a DOI URL or plain DOI identifier"""
        if not doi_input:
            return None

        # If it's a DOI URL (https://doi.org/10.xxxx/yyyy or http://dx.doi.org/10.xxxx/yyyy)
        doi_url_pattern = r'https?://(?:dx\.)?doi\.org/(.+)'
        url_match = re.match(doi_url_pattern, doi_input, re.IGNORECASE)
        if url_match:
            doi_identifier = url_match.group(1)
        else:
            # Assume it's already a DOI identifier
            doi_identifier = doi_input

        # Validate the extracted DOI identifier
        if self._is_valid_doi_identifier(doi_identifier):
            return doi_identifier

        return None

    def _is_valid_doi_identifier(self, doi: str) -> bool:
        """Validate DOI identifier format (just the 10.xxxx/yyyy part)"""
        # DOI identifier pattern: starts with 10., followed by registrant code, slash, suffix
        doi_pattern = r'^10\.\d{4,}/[^\s]+$'
        return bool(re.match(doi_pattern, doi, re.IGNORECASE))

    def _is_valid_url(self, url: str) -> bool:
        """Validate URL format"""
        try:
            parsed = urlparse(url)
            return bool(parsed.scheme and parsed.netloc)
        except:
            return False

    def _safe_int(self, value: Optional[str]) -> Optional[int]:
        """Safely convert string to integer"""
        if not value or value.strip() == '':
            return None
        try:
            return int(value.strip())
        except (ValueError, TypeError):
            return None

    def _safe_bool(self, value: Optional[str]) -> bool:
        """Safely convert string to boolean"""
        if not value:
            return False
        value_lower = value.strip().lower()
        return value_lower in ('true', '1', 'yes', 't')

    def _safe_openalex_id(self, value: Optional[str]) -> Optional[int]:
        """Safely convert OpenAlex ID to BIGINT"""
        if not value or value.strip() == '':
            return None

        value = value.strip()

        try:
            # If it's already a number, convert directly
            if value.isdigit():
                return int(value)

            # Handle full URLs like "https://openalex.org/W1982051859"
            if 'openalex.org/W' in value:
                numeric_part = value.split('openalex.org/W')[1]
                return int(numeric_part)

            # Handle W-prefixed IDs like "W1982051859"
            elif value.startswith('W'):
                return int(value[1:])  # Remove the 'W' prefix

            # Try to extract any numeric part
            import re
            numeric_match = re.search(r'\d+', value)
            if numeric_match:
                return int(numeric_match.group())

            return None

        except (ValueError, TypeError, IndexError):
            logger.warning(f"Could not convert OpenAlex ID to integer: {value}")
            return None

    def _calculate_url_quality_score(self, row: Dict[str, str]) -> int:
        """Calculate quality score for URL based on various factors"""
        score = 50  # Base score
        
        # PDF URL bonus (MOST IMPORTANT!)
        pdf_url = row.get('pdf_url', '').strip()
        if pdf_url:
            score += 25  # Big bonus for direct PDF access
        
        # Location type scoring
        location_type = row.get('location_type', '').lower()
        if location_type == 'best_oa':
            score += 20
        elif location_type == 'primary':
            score += 15
        elif location_type == 'alternate':
            score += 5
        
        # Open access scoring
        if self._safe_bool(row.get('is_oa', '')):
            score += 10
            
            oa_status = row.get('oa_status', '').lower()
            if oa_status == 'gold':
                score += 10
            elif oa_status == 'green':
                score += 5
        
        # Version scoring
        version = row.get('version', '').lower()
        if 'published' in version:
            score += 10
        elif 'accepted' in version:
            score += 5
        
        # Host type scoring
        host_type = row.get('host_type', '').lower()
        if host_type in ['journal', 'doaj_journal']:
            score += 10
        elif host_type == 'repository':
            score += 8
        elif host_type == 'preprint_server':
            score += 5
        
        # Domain reputation (simple heuristic)
        url = row.get('url', '').lower()
        reputable_domains = [
            'pubmed', 'pmc', 'doi.org', 'nature.com', 'science.org',
            'elsevier.com', 'springer.com', 'wiley.com', 'arxiv.org',
            'biorxiv.org', 'medrxiv.org'
        ]
        if any(domain in url for domain in reputable_domains):
            score += 5
        
        return min(100, max(0, score))  # Clamp between 0-100

    def read_csv_in_batches_optimized(self, connection) -> Generator[Tuple[List[Dict[str, Any]], int], None, None]:
        """
        PERFORMANCE OPTIMIZED: Memory-efficient generator that yields batches of validated rows.

        Reuses database connection for lookup operations to avoid connection overhead.
        """
        if not self.csv_file.exists():
            raise FileNotFoundError(f"CSV file not found: {self.csv_file}")

        logger.info(f"Reading CSV file: {self.csv_file}")

        # Check if file is empty or too small
        file_size = self.csv_file.stat().st_size
        if file_size == 0:
            raise ValueError("CSV file is empty")

        # Use cached line count to avoid double counting
        line_count = self.total_csv_rows + 1  # Add 1 for header

        if line_count <= 1:
            raise ValueError(f"CSV file has no data rows (only {line_count} line(s) found)")

        logger.info(f"CSV file has {line_count:,} lines (using cached count)")

        # Calculate resume information
        if self.start_row > 0:
            logger.info(f"Resuming from row {self.start_row} (skipping {self.start_row} rows)")

        # Use comma delimiter directly as specified
        delimiter = ','
        logger.info("Using comma as CSV delimiter")

        with open(self.csv_file, 'r', encoding='utf-8', errors='replace') as csvfile:
            try:
                reader = csv.DictReader(csvfile, delimiter=delimiter)

                # Validate CSV structure by checking first row
                try:
                    first_row = next(reader)
                    csvfile.seek(0)
                    reader = csv.DictReader(csvfile, delimiter=delimiter)  # Reset reader

                    # Check if we have expected columns
                    expected_columns = ['doi', 'url']  # Minimum required columns
                    missing_columns = [col for col in expected_columns if col not in first_row]
                    if missing_columns:
                        logger.warning(f"Missing expected columns: {missing_columns}")
                        logger.info(f"Available columns: {list(first_row.keys())}")
                        raise ValueError(f"CSV is missing required columns: {missing_columns}")

                    # Log the first row for debugging
                    logger.info(f"First row sample: {first_row}")

                except StopIteration:
                    raise ValueError("CSV file appears to be empty or has no data rows")

                # Initialize progress bar
                remaining_rows = line_count - 1 - self.start_row  # Total rows minus header minus already processed
                progress_bar = tqdm(
                    total=remaining_rows,
                    desc="Processing CSV",
                    unit="rows",
                    unit_scale=True
                )

                current_batch = []
                batch_count = 0
                valid_rows = 0
                invalid_rows = 0
                current_row = 0

                try:
                    for row in reader:
                        # Skip rows if resuming
                        if current_row < self.start_row:
                            current_row += 1
                            continue

                        # Pass connection to avoid creating new connections for lookups
                        cleaned_row = self.validate_and_clean_row(row, connection)

                        if cleaned_row:
                            current_batch.append(cleaned_row)
                            valid_rows += 1
                        else:
                            invalid_rows += 1
                            self.stats['rows_skipped'] += 1

                        self.stats['total_rows_processed'] += 1
                        current_row += 1
                        progress_bar.update(1)

                        # When batch is full, yield it and start a new one
                        if len(current_batch) >= self.batch_size:
                            batch_count += 1
                            logger.debug(f"Yielding batch {batch_count} with {len(current_batch)} rows")
                            yield (current_batch, current_row)
                            current_batch = []

                    # Yield final batch if not empty
                    if current_batch:
                        batch_count += 1
                        logger.debug(f"Yielding final batch {batch_count} with {len(current_batch)} rows")
                        yield (current_batch, current_row)

                finally:
                    progress_bar.close()
                    logger.info(f"CSV processing complete: {valid_rows} valid rows, {invalid_rows} invalid rows skipped")

            except Exception as e:
                logger.error(f"Error processing CSV: {e}")
                raise

    # Keep the old method for compatibility
    def read_csv_in_batches(self) -> Generator[Tuple[List[Dict[str, Any]], int], None, None]:
        """Legacy CSV reader - creates new connections for each lookup."""
        if not self.csv_file.exists():
            raise FileNotFoundError(f"CSV file not found: {self.csv_file}")

        logger.info(f"Reading CSV file: {self.csv_file}")

        # Use the old implementation without connection reuse
        with open(self.csv_file, 'r', encoding='utf-8', errors='replace') as csvfile:
            reader = csv.DictReader(csvfile, delimiter=',')

            current_batch = []
            current_row = 0

            for row in reader:
                if current_row < self.start_row:
                    current_row += 1
                    continue

                cleaned_row = self.validate_and_clean_row(row)  # No connection passed

                if cleaned_row:
                    current_batch.append(cleaned_row)

                current_row += 1

                if len(current_batch) >= self.batch_size:
                    yield (current_batch, current_row)
                    current_batch = []

            if current_batch:
                yield (current_batch, current_row)

    def _deduplicate_batch(self, batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Remove duplicate (doi, url) pairs within a batch, keeping the record with highest quality score.

        Args:
            batch: List of validated row dictionaries

        Returns:
            Deduplicated list of row dictionaries
        """
        if not batch:
            return batch

        # Track duplicates using (doi, url) as key
        unique_records = {}
        duplicates_found = 0

        for row in batch:
            key = (row['doi'], row['url'])

            if key in unique_records:
                duplicates_found += 1
                existing_record = unique_records[key]

                # Smart deduplication: prefer records with PDF URLs, then higher quality scores
                existing_has_pdf = existing_record['pdf_url'] is not None and existing_record['pdf_url'].strip() != ''
                current_has_pdf = row['pdf_url'] is not None and row['pdf_url'].strip() != ''
                existing_score = existing_record['url_quality_score']
                current_score = row['url_quality_score']

                should_replace = False
                reason = ""

                if current_has_pdf and not existing_has_pdf:
                    # New record has PDF URL, existing doesn't - prefer new
                    should_replace = True
                    reason = f"new record has PDF URL (existing doesn't)"
                elif existing_has_pdf and not current_has_pdf:
                    # Existing has PDF URL, new doesn't - keep existing
                    should_replace = False
                    reason = f"existing record has PDF URL (new doesn't)"
                elif current_has_pdf and existing_has_pdf:
                    # Both have PDF URLs - merge and prefer higher quality score
                    should_replace = current_score > existing_score
                    reason = f"both have PDF URLs, quality scores: {current_score} vs {existing_score}"
                else:
                    # Neither has PDF URL - prefer higher quality score
                    should_replace = current_score > existing_score
                    reason = f"neither has PDF URL, quality scores: {current_score} vs {existing_score}"

                if should_replace:
                    # If we're replacing but the existing record had a PDF URL and new doesn't, preserve it
                    if existing_has_pdf and not current_has_pdf:
                        row['pdf_url'] = existing_record['pdf_url']
                        reason += " (preserved existing PDF URL)"

                    unique_records[key] = row
                    logger.debug(f"Replaced duplicate (doi={row['doi']}, url={row['url'][:50]}...) - {reason}")
                else:
                    # If we're keeping existing but new record has PDF URL and existing doesn't, update it
                    if current_has_pdf and not existing_has_pdf:
                        existing_record['pdf_url'] = row['pdf_url']
                        reason += " (updated with new PDF URL)"

                    logger.debug(f"Kept existing duplicate (doi={row['doi']}, url={row['url'][:50]}...) - {reason}")
            else:
                unique_records[key] = row

        if duplicates_found > 0:
            logger.debug(f"Deduplicated batch: removed {duplicates_found} duplicate (doi, url) pairs")
            self.stats['batch_duplicates'] += duplicates_found

        return list(unique_records.values())

    def insert_batch_optimized(self, batch: List[Dict[str, Any]], connection) -> Tuple[int, int]:
        """
        PERFORMANCE OPTIMIZED: Insert a batch of rows using true bulk operations.

        This method uses execute_many for much better performance than row-by-row inserts.
        """
        if not batch:
            return 0, 0

        # Deduplicate the batch to prevent constraint violations
        deduplicated_batch = self._deduplicate_batch(batch)

        if len(deduplicated_batch) != len(batch):
            logger.debug(f"Batch size reduced from {len(batch)} to {len(deduplicated_batch)} after deduplication")

        # Prepare data for bulk insert with normalized structure
        insert_data = []
        for row in deduplicated_batch:
            insert_data.append((
                row['doi'],
                row['url'],
                row.get('pdf_url'),
                row.get('openalex_id'),
                row.get('title'),
                row.get('publication_year'),
                row.get('location_type', 'p'),
                row.get('version'),
                row.get('license_id'),
                row.get('host_type_id'),
                row.get('oa_status_id'),
                row.get('is_oa', False),
                row.get('work_type_id'),
                row.get('is_retracted', False),
                row.get('url_quality_score', 50)
            ))

        rows_inserted = 0
        rows_updated = 0

        try:
            with connection.cursor() as cur:
                # Use execute_many for true bulk insert performance
                # Split into reasonable chunks to avoid memory issues
                chunk_size = 1000  # Much larger chunks for better performance

                for i in range(0, len(insert_data), chunk_size):
                    chunk = insert_data[i:i+chunk_size]
                    chunk_num = i//chunk_size + 1
                    total_chunks = (len(insert_data) + chunk_size - 1) // chunk_size

                    logger.debug(f"Processing chunk {chunk_num}/{total_chunks} ({len(chunk)} rows)")

                    # Use execute_many for bulk insert with simplified conflict resolution
                    try:
                        # First, try bulk insert without conflict resolution for new records
                        cur.executemany("""
                            INSERT INTO unpaywall.doi_urls (
                                doi, url, pdf_url, openalex_id, title, publication_year, location_type,
                                version, license_id, host_type_id, oa_status_id, is_oa, work_type_id, is_retracted, url_quality_score
                            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                            ON CONFLICT (doi, url) DO NOTHING
                        """, chunk)

                        # Count how many were actually inserted
                        chunk_inserted = cur.rowcount
                        rows_inserted += chunk_inserted

                        # For conflicts, handle updates separately if needed
                        conflicts = len(chunk) - chunk_inserted
                        if conflicts > 0:
                            logger.debug(f"Chunk {chunk_num}: {chunk_inserted} inserted, {conflicts} conflicts (skipped)")
                            rows_updated += conflicts  # Count conflicts as updates for stats

                        # Commit this chunk
                        connection.commit()

                    except psycopg2.Error as e:
                        connection.rollback()
                        logger.warning(f"Bulk insert failed for chunk {chunk_num}, falling back to row-by-row: {e}")

                        # Fallback to row-by-row for this chunk only
                        chunk_inserted, chunk_updated = self._insert_chunk_row_by_row(chunk, cur, connection)
                        rows_inserted += chunk_inserted
                        rows_updated += chunk_updated

                logger.debug(f"Batch complete: {rows_inserted} reported inserted, {rows_updated} conflicts")
                return rows_inserted, rows_updated

        except psycopg2.Error as e:
            connection.rollback()
            logger.error(f"Optimized batch insert failed: {e}")
            raise

    def _insert_chunk_row_by_row(self, chunk: List[tuple], cursor, connection) -> Tuple[int, int]:
        """Fallback method for row-by-row insertion when bulk insert fails."""
        chunk_inserted = 0
        chunk_updated = 0

        for row_data in chunk:
            try:
                cursor.execute("""
                    INSERT INTO unpaywall.doi_urls (
                        doi, url, pdf_url, openalex_id, title, publication_year, location_type,
                        version, license_id, host_type_id, oa_status_id, is_oa, work_type_id, is_retracted, url_quality_score
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (doi, url) DO UPDATE SET
                        pdf_url = CASE
                            WHEN EXCLUDED.pdf_url IS NOT NULL AND EXCLUDED.pdf_url != ''
                            THEN EXCLUDED.pdf_url
                            ELSE unpaywall.doi_urls.pdf_url
                        END,
                        updated_at = CURRENT_TIMESTAMP
                    RETURNING (xmax = 0) AS inserted
                """, row_data)

                result = cursor.fetchone()
                if result and result[0]:  # xmax = 0 means inserted
                    chunk_inserted += 1
                else:
                    chunk_updated += 1

            except Exception as e:
                connection.rollback()
                logger.error(f"Error inserting single row: {e}")
                continue

        connection.commit()
        return chunk_inserted, chunk_updated

    # Keep the old method as fallback
    def insert_batch(self, batch: List[Dict[str, Any]], connection) -> Tuple[int, int]:
        """Legacy batch insert method - kept for compatibility."""
        return self.insert_batch_optimized(batch, connection)



    def print_final_stats(self):
        """Print import statistics including normalized database metrics"""
        duration = self.stats['end_time'] - self.stats['start_time']

        print("\n" + "=" * 60)
        print("DOI-URL IMPORT COMPLETED (NORMALIZED DATABASE)")
        print("=" * 60)
        print(f"Import ID: {self.import_id}")
        if self.stats['resumed_from_row'] > 0:
            print(f"Resumed from row: {self.stats['resumed_from_row']:,}")
        print(f"Total rows processed: {self.stats['total_rows_processed']:,}")
        print(f"Rows inserted: {self.stats['rows_inserted']:,}")
        print(f"Rows updated: {self.stats['rows_updated']:,}")
        print(f"Rows skipped (invalid): {self.stats['rows_skipped']:,}")
        print(f"Batch duplicates removed: {self.stats['batch_duplicates']:,}")
        print(f"Invalid URLs filtered: {self.stats['invalid_urls']:,}")

        # Lookup cache statistics
        total_cache_entries = sum(len(cache) for cache in self.lookup_caches.values())
        cache_hit_rate = (self.stats['cache_hits'] / (self.stats['cache_hits'] + self.stats['cache_misses']) * 100) if (self.stats['cache_hits'] + self.stats['cache_misses']) > 0 else 0
        print(f"Lookup cache entries: {total_cache_entries:,}")
        print(f"Cache hit rate: {cache_hit_rate:.1f}% ({self.stats['cache_hits']:,} hits, {self.stats['cache_misses']:,} misses)")

        print(f"Import duration: {duration:.1f} seconds")
        if duration > 0:
            print(f"Processing rate: {self.stats['total_rows_processed']/duration:.1f} rows/sec")
        print("=" * 60)

    def run_import(self):
        """Execute the complete import process with normalized database support"""
        logger.info(">>> Starting DOI-URL import process with normalized database...")
        self.stats['start_time'] = time.time()

        try:
            # Create schema if requested
            if self.create_tables:
                schema_start = time.time()
                logger.info("Creating database schema...")
                self.create_schema()
                schema_time = time.time() - schema_start
                logger.info(f"Schema creation completed in {schema_time:.1f} seconds")

            # Setup resume state
            resume_start = time.time()
            logger.info("Setting up resume state...")
            self._setup_resume_state()
            resume_time = time.time() - resume_start
            logger.info(f"Resume state setup completed in {resume_time:.1f} seconds")

            # Preload lookup table caches for performance
            cache_start = time.time()
            logger.info("Preloading lookup table caches...")
            self.preload_lookup_caches()
            cache_time = time.time() - cache_start
            logger.info(f"Cache preloading completed in {cache_time:.1f} seconds")

            # Disable indexes for bulk import performance
            index_start = time.time()
            logger.info("Disabling indexes for bulk import...")
            self.disable_indexes_for_bulk_import()
            index_time = time.time() - index_start
            logger.info(f"Index disabling completed in {index_time:.1f} seconds")

            # Test database connection and insertion
            test_start = time.time()
            logger.info("Testing database connection...")
            test_success = self.test_database_connection()
            test_time = time.time() - test_start
            logger.info(f"Database test completed in {test_time:.1f} seconds")

            if not test_success:
                logger.error("Database test failed - cannot proceed with import")
                self._complete_import(success=False, error_message="Database test failed")
                return

            # PERFORMANCE OPTIMIZED: Process CSV in batches using optimized generator
            logger.info("Reading CSV file and processing batches with optimized methods...")

            # Use single connection for entire import to avoid connection overhead
            with self.connect_db() as conn:
                # Get initial count
                with conn.cursor() as cur:
                    cur.execute("SELECT COUNT(*) FROM unpaywall.doi_urls")
                    initial_count = cur.fetchone()[0]
                    logger.info(f"Initial database count: {initial_count} rows")

                # Use optimized batch generator that reuses the connection
                batch_generator = self.read_csv_in_batches_optimized(conn)

                batch_count = 0
                total_rows_processed = 0

                # Process each batch as it comes from the generator
                for batch_data in batch_generator:
                    batch_start_time = time.time()
                    batch, current_row = batch_data
                    batch_count += 1
                    total_rows_processed += len(batch)

                    logger.info(f"Processing batch {batch_count} with {len(batch)} rows (total processed: {total_rows_processed}, current row: {current_row})")

                    # Now try the full batch with optimized insert
                    try:
                        # Insert batch using optimized method
                        insert_start = time.time()
                        rows_inserted, rows_updated = self.insert_batch_optimized(batch, conn)
                        insert_time = time.time() - insert_start

                        self.stats['rows_inserted'] += rows_inserted
                        self.stats['rows_updated'] += rows_updated

                        batch_total_time = time.time() - batch_start_time
                        rows_per_sec = len(batch) / batch_total_time if batch_total_time > 0 else 0

                        logger.info(f"Batch {batch_count} result: {rows_inserted} inserted, {rows_updated} conflicts")
                        logger.info(f"Batch {batch_count} timing: {batch_total_time:.2f}s total ({insert_time:.2f}s insert) - {rows_per_sec:.1f} rows/sec")

                        # Update progress tracking
                        self._update_import_progress(current_row, batch_count)

                    except Exception as e:
                        logger.error(f"Error processing batch {batch_count}: {e}")
                        self._complete_import(success=False, error_message=str(e))
                        raise

                    # Check database count and performance periodically (less frequently to reduce overhead)
                    # if batch_count % 50 == 0:
                    #     with conn.cursor() as cur:
                    #         cur.execute("SELECT COUNT(*) FROM unpaywall.doi_urls")
                    #         current_count = cur.fetchone()[0]
                    #         elapsed_time = time.time() - self.stats['start_time']
                    #         rows_per_sec = total_rows_processed / elapsed_time if elapsed_time > 0 else 0
                    #         logger.info(f"Progress check: {current_count} rows in database (+{current_count - initial_count} from start)")
                    #         logger.info(f"Overall processing rate: {rows_per_sec:.1f} rows/sec after {elapsed_time:.1f} seconds")

                    # Optional: Stop after a few batches in debug mode to check what's happening
                    # Commented out to allow full import even in debug mode
                    # if batch_count >= 5 and logger.level == logging.DEBUG:
                    #     logger.debug("Debug mode: stopping after 5 batches for analysis")
                    #     break
                
                # Get final count
                with conn.cursor() as cur:
                    cur.execute("SELECT COUNT(*) FROM unpaywall.doi_urls")
                    final_count = cur.fetchone()[0]
                    logger.info(f"Final database count: {final_count} rows (+{final_count - initial_count} from start)")

            # Recreate indexes after bulk import
            self.recreate_indexes_after_import()

            # Mark import as completed
            self._complete_import(success=True)

            self.stats['end_time'] = time.time()
            self.print_final_stats()

        except Exception as e:
            logger.error(f"Import failed: {e}")
            # Try to recreate indexes even if import failed
            try:
                self.recreate_indexes_after_import()
            except Exception as index_error:
                logger.warning(f"Failed to recreate indexes after failed import: {index_error}")

            self._complete_import(success=False, error_message=str(e))
            raise

    

    def list_import_history(self, limit: int = 10):
        """List recent import history for debugging"""
        try:
            with self.connect_db() as conn:
                with conn.cursor(cursor_factory=RealDictCursor) as cur:
                    cur.execute("""
                        SELECT import_id, csv_file_path, status, processed_rows, total_rows,
                               start_time, end_time, error_message
                        FROM unpaywall.import_progress
                        ORDER BY start_time DESC
                        LIMIT %s
                    """, (limit,))

                    imports = cur.fetchall()

                    if imports:
                        print(f"\nRecent import history (last {len(imports)} imports):")
                        print("-" * 100)
                        for imp in imports:
                            progress_pct = (imp['processed_rows'] / imp['total_rows'] * 100) if imp['total_rows'] > 0 else 0
                            print(f"ID: {imp['import_id']}")
                            print(f"  File: {imp['csv_file_path']}")
                            print(f"  Status: {imp['status']}")
                            print(f"  Progress: {imp['processed_rows']:,}/{imp['total_rows']:,} ({progress_pct:.1f}%)")
                            print(f"  Started: {imp['start_time']}")
                            if imp['end_time']:
                                print(f"  Ended: {imp['end_time']}")
                            if imp['error_message']:
                                print(f"  Error: {imp['error_message']}")
                            print()
                    else:
                        print("No import history found.")

        except Exception as e:
            logger.error(f"Error listing import history: {e}")

    def test_database_connection(self) -> bool:
        """Test database connection and basic operations"""
        try:
            with self.connect_db() as conn:
                with conn.cursor() as cur:
                    # Test basic connection
                    cur.execute("SELECT 1")
                    result = cur.fetchone()
                    if result[0] != 1:
                        logger.error("Database connection test failed")
                        return False

                    # Test if unpaywall schema exists
                    cur.execute("""
                        SELECT EXISTS (
                            SELECT FROM information_schema.schemata
                            WHERE schema_name = 'unpaywall'
                        )
                    """)
                    schema_exists = cur.fetchone()[0]
                    if not schema_exists:
                        logger.warning("unpaywall schema does not exist - will be created")

                    # Test if tables exist
                    cur.execute("""
                        SELECT EXISTS (
                            SELECT FROM information_schema.tables
                            WHERE table_schema = 'unpaywall' AND table_name = 'doi_urls'
                        )
                    """)
                    table_exists = cur.fetchone()[0]
                    if not table_exists:
                        logger.warning("unpaywall.doi_urls table does not exist - will be created")

                    logger.info("Database connection test successful")
                    return True

        except Exception as e:
            logger.error(f"Database connection test failed: {e}")
            return False

    def insert_single_row(self, row: Dict[str, Any], connection) -> bool:
        """Insert a single row for testing purposes with normalized structure"""
        try:
            with connection.cursor() as cur:
                cur.execute("""
                    INSERT INTO unpaywall.doi_urls (
                        doi, url, pdf_url, openalex_id, title, publication_year, location_type,
                        version, license_id, host_type_id, oa_status_id, is_oa, work_type_id, is_retracted, url_quality_score
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (doi, url) DO UPDATE SET
                        pdf_url = CASE
                            WHEN EXCLUDED.pdf_url IS NOT NULL AND EXCLUDED.pdf_url != ''
                            THEN EXCLUDED.pdf_url
                            ELSE unpaywall.doi_urls.pdf_url
                        END,
                        updated_at = CURRENT_TIMESTAMP
                """, (
                    row['doi'],
                    row['url'],
                    row.get('pdf_url'),
                    row.get('openalex_id'),
                    row.get('title'),
                    row.get('publication_year'),
                    row.get('location_type', 'p'),  # Default to 'p' for primary
                    row.get('version'),
                    row.get('license_id'),  # Foreign key ID
                    row.get('host_type_id'),  # Foreign key ID
                    row.get('oa_status_id'),  # Foreign key ID
                    row.get('is_oa', False),
                    row.get('work_type_id'),  # Foreign key ID
                    row.get('is_retracted', False),
                    row.get('url_quality_score', 50)
                ))
                connection.commit()
                return True

        except Exception as e:
            logger.error(f"Single row insert failed: {e}")
            connection.rollback()
            return False


def main():
    print(f"DOI URL Importer v{__version__}")
    # Check for .env configuration first
    env_config = load_env_config()
    env_available = env_config is not None

    # Create argument parser with conditional required arguments
    parser = argparse.ArgumentParser(
        description='Import DOI-URL CSV into PostgreSQL',
        epilog='Database credentials can be provided via command line arguments or .env file. '
               'Command line arguments take precedence over .env file values.'
    )
    parser.add_argument('--csv-file', required=True,
                        help='Path to CSV file with DOI-URL mappings')

    # Database arguments - make them optional if .env is available
    parser.add_argument('--db-host',
                        default=env_config.get('host', 'localhost') if env_available else 'localhost',
                        help=f'PostgreSQL host (default: {"from .env" if env_available else "localhost"})')
    parser.add_argument('--db-port', type=int,
                        default=env_config.get('port', 5432) if env_available else 5432,
                        help=f'PostgreSQL port (default: {"from .env" if env_available else "5432"})')
    parser.add_argument('--db-name',
                        required=not env_available,
                        default=env_config.get('database') if env_available else None,
                        help=f'PostgreSQL database name{"" if env_available else " (required)"}')
    parser.add_argument('--db-user',
                        required=not env_available,
                        default=env_config.get('user') if env_available else None,
                        help=f'PostgreSQL username{"" if env_available else " (required)"}')
    parser.add_argument('--db-password',
                        required=not env_available,
                        default=env_config.get('password') if env_available else None,
                        help=f'PostgreSQL password{"" if env_available else " (required)"}')

    parser.add_argument('--batch-size', type=int, default=25000,
                        help='Batch size for bulk inserts (default: 25000, increase for better performance)')
    parser.add_argument('--skip-create-tables', action='store_true',
                        help='Skip table creation (tables already exist)')
    parser.add_argument('--test-only', action='store_true',
                        help='Only test database connection and exit')
    parser.add_argument('--debug', action='store_true',
                        help='Enable debug logging')
    parser.add_argument('--resume', action='store_true',
                        help='Resume from previous incomplete import')
    parser.add_argument('--list-imports', action='store_true',
                        help='List recent import history and exit')

    args = parser.parse_args()

    # Set debug logging if requested
    if args.debug:
        logger.setLevel(logging.DEBUG)
        for handler in logger.handlers:
            handler.setLevel(logging.DEBUG)

    # Build database configuration, prioritizing command line args over .env
    db_config = {}

    # Use command line arguments if provided, otherwise fall back to .env values
    db_config['host'] = args.db_host
    db_config['port'] = args.db_port
    db_config['database'] = args.db_name
    db_config['user'] = args.db_user
    db_config['password'] = args.db_password

    # Validate that all required database parameters are available
    required_params = ['host', 'port', 'database', 'user', 'password']
    missing_params = [param for param in required_params if not db_config.get(param)]

    if missing_params:
        logger.error(f"Missing required database parameters: {missing_params}")
        if env_available:
            logger.error("Parameters not found in command line arguments or .env file")
        else:
            logger.error("No .env file found and parameters not provided via command line")
        sys.exit(1)

    # Log configuration source
    if env_available:
        logger.info("Database configuration loaded from .env file (command line arguments take precedence)")
    else:
        logger.info("Database configuration loaded from command line arguments")

    logger.info(f"Connecting to database: {db_config['user']}@{db_config['host']}:{db_config['port']}/{db_config['database']}")

    print("=" * 60)
    print("Initializing DOI-URL import process with normalized database...")
    print("=" * 60)

    # Create importer
    importer = DOIURLImporter(
        db_config=db_config,
        csv_file=args.csv_file,
        batch_size=args.batch_size,
        create_tables=not args.skip_create_tables,
        resume=args.resume
    )

    # List import history if requested
    if args.list_imports:
        importer.list_import_history()
        sys.exit(0)
    print("Starting import...")
    # Run the import
    importer.run_import()


if __name__ == '__main__':
    main()
