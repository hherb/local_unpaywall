#!/usr/bin/env python3
"""
DOI-URL CSV Importer for PostgreSQL
===================================

This script imports the CSV file generated by the OpenAlex URL extractor
into PostgreSQL tables for efficient DOI-URL mapping.

Features:
- Efficient batch processing with configurable batch sizes
- Data validation and cleaning
- Duplicate handling with upsert capability
- Progress tracking and logging
- Resume functionality for interrupted imports
- URL quality scoring based on source characteristics

Usage:
    python import_doi_urls.py --csv-file doi_urls.csv --db-name biomedical 
                              --db-user myuser --db-password mypass
"""

import argparse
import csv
import logging
import sys
import time
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Generator
from urllib.parse import urlparse
import re

try:
    from tqdm import tqdm
except ImportError:
    print("Error: tqdm not found. Install with: pip install tqdm")
    sys.exit(1)

try:
    import psycopg2
    from psycopg2 import sql
    from psycopg2.extras import execute_values, RealDictCursor
except ImportError:
    print("Error: psycopg2 not found. Install with: pip install psycopg2-binary")
    sys.exit(1)

# Set up logging
logging.basicConfig(
    level=logging.ERROR,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('doi_url_import.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DOIURLImporter:
    def __init__(self, db_config: Dict[str, str], csv_file: str, 
                 batch_size: int = 10000, create_tables: bool = True):
        self.db_config = db_config
        self.csv_file = Path(csv_file)
        self.batch_size = batch_size
        self.create_tables = create_tables
        
        # Statistics
        self.stats = {
            'total_rows_processed': 0,
            'rows_inserted': 0,
            'rows_updated': 0,
            'rows_skipped': 0,
            'duplicate_dois': 0,
            'batch_duplicates': 0,  # Duplicates found within batches
            'invalid_urls': 0,
            'start_time': None,
            'end_time': None
        }

    def connect_db(self):
        """Establish database connection"""
        try:
            conn = psycopg2.connect(**self.db_config)
            return conn
        except psycopg2.Error as e:
            logger.error(f"Database connection failed: {e}")
            raise

    def create_schema(self):
        """Create the DOI-URL mapping tables with error handling"""
        logger.info("Creating DOI-URL schema...")
        
        # First check if tables already exist
        with self.connect_db() as conn:
            with conn.cursor() as cur:
                cur.execute("""
                    SELECT EXISTS (
                        SELECT FROM information_schema.tables 
                        WHERE table_name = 'doi_urls'
                    )
                """)
                table_exists = cur.fetchone()[0]
                
                if table_exists:
                    logger.info("doi_urls table already exists")
                    # Check if it has the unique constraint
                    cur.execute("""
                        SELECT COUNT(*) FROM pg_constraint 
                        WHERE conname = 'unique_doi_url' AND conrelid = 'doi_urls'::regclass
                    """)
                    has_constraint = cur.fetchone()[0] > 0
                    
                    if not has_constraint:
                        logger.warning("unique_doi_url constraint missing - will add it")
                else:
                    logger.info("doi_urls table does not exist - will create it")
        
        # Create tables with explicit transaction handling
        with self.connect_db() as conn:
            with conn.cursor() as cur:
                try:
                    # Create main table if it doesn't exist
                    if not table_exists:
                        cur.execute("""
                        CREATE TABLE IF NOT EXISTS doi_urls (
                            id BIGSERIAL PRIMARY KEY,
                            doi TEXT NOT NULL,
                            url TEXT NOT NULL,
                            pdf_url TEXT,
                            openalex_id TEXT,
                            title TEXT,
                            publication_year INTEGER,
                            location_type TEXT NOT NULL,
                            version TEXT,
                            license TEXT,
                            host_type TEXT,
                            oa_status TEXT,
                            is_oa BOOLEAN DEFAULT FALSE,
                            url_quality_score INTEGER DEFAULT 50,
                            last_verified TIMESTAMP,
                            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                        )
                        """)
                        logger.info("Created doi_urls table")
                    
                    # Create indexes
                    cur.execute("CREATE INDEX IF NOT EXISTS idx_doi_urls_doi ON doi_urls(doi)")
                    cur.execute("CREATE INDEX IF NOT EXISTS idx_doi_urls_url ON doi_urls(url)")
                    
                    # Add unique constraint if it doesn't exist
                    if not table_exists or not has_constraint:
                        try:
                            cur.execute("ALTER TABLE doi_urls ADD CONSTRAINT unique_doi_url UNIQUE(doi, url)")
                            logger.info("Added unique_doi_url constraint")
                        except psycopg2.Error as e:
                            if "already exists" in str(e):
                                logger.info("unique_doi_url constraint already exists")
                            else:
                                raise
                    
                    # Create metadata table
                    cur.execute("""
                    CREATE TABLE IF NOT EXISTS doi_metadata (
                        doi TEXT PRIMARY KEY,
                        openalex_id TEXT UNIQUE,
                        title TEXT,
                        publication_year INTEGER,
                        work_type TEXT,
                        is_retracted BOOLEAN DEFAULT FALSE,
                        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                    )
                    """)
                    
                    # Commit all changes
                    conn.commit()
                    logger.info("Schema created successfully")
                    
                    # Verify tables exist
                    cur.execute("SELECT COUNT(*) FROM doi_urls")
                    count = cur.fetchone()[0]
                    logger.info(f"doi_urls table exists with {count} rows")
                    
                except psycopg2.Error as e:
                    conn.rollback()
                    logger.error(f"Schema creation failed: {e}")
                    raise

    def validate_and_clean_row(self, row: Dict[str, str]) -> Optional[Dict[str, Any]]:
        """Validate and clean a single CSV row"""

        # Required fields
        doi_raw = row.get('doi', '').strip()
        url = row.get('url', '').strip()
        location_type = row.get('location_type', '').strip()

        if not doi_raw or not url or not location_type:
            return None

        # Extract DOI identifier from DOI URL if needed
        doi_identifier = self._extract_doi_identifier(doi_raw)
        if not doi_identifier:
            logger.warning(f"Invalid DOI format: {doi_raw}")
            return None

        # Validate URL
        if not self._is_valid_url(url):
            self.stats['invalid_urls'] += 1
            logger.warning(f"Invalid URL: {url}")
            return None

        # Clean and convert data
        cleaned_row = {
            'doi': doi_identifier.lower(),  # Store just the DOI identifier, normalized
            'url': url,
            'pdf_url': row.get('pdf_url', '').strip() or None,
            'openalex_id': row.get('openalex_id', '').strip() or None,
            'title': row.get('title', '').strip() or None,
            'publication_year': self._safe_int(row.get('publication_year', '')),
            'location_type': location_type.lower(),
            'version': row.get('version', '').strip() or None,
            'license': row.get('license', '').strip() or None,
            'host_type': row.get('host_type', '').strip() or None,
            'oa_status': row.get('oa_status', '').strip() or None,
            'is_oa': self._safe_bool(row.get('is_oa', '')),
            'url_quality_score': self._calculate_url_quality_score(row)
        }

        return cleaned_row

    def _extract_doi_identifier(self, doi_input: str) -> Optional[str]:
        """Extract DOI identifier from either a DOI URL or plain DOI identifier"""
        if not doi_input:
            return None

        # If it's a DOI URL (https://doi.org/10.xxxx/yyyy or http://dx.doi.org/10.xxxx/yyyy)
        doi_url_pattern = r'https?://(?:dx\.)?doi\.org/(.+)'
        url_match = re.match(doi_url_pattern, doi_input, re.IGNORECASE)
        if url_match:
            doi_identifier = url_match.group(1)
        else:
            # Assume it's already a DOI identifier
            doi_identifier = doi_input

        # Validate the extracted DOI identifier
        if self._is_valid_doi_identifier(doi_identifier):
            return doi_identifier

        return None

    def _is_valid_doi_identifier(self, doi: str) -> bool:
        """Validate DOI identifier format (just the 10.xxxx/yyyy part)"""
        # DOI identifier pattern: starts with 10., followed by registrant code, slash, suffix
        doi_pattern = r'^10\.\d{4,}/[^\s]+$'
        return bool(re.match(doi_pattern, doi, re.IGNORECASE))

    def _is_valid_url(self, url: str) -> bool:
        """Validate URL format"""
        try:
            parsed = urlparse(url)
            return bool(parsed.scheme and parsed.netloc)
        except:
            return False

    def _safe_int(self, value: Optional[str]) -> Optional[int]:
        """Safely convert string to integer"""
        if not value or value.strip() == '':
            return None
        try:
            return int(value.strip())
        except (ValueError, TypeError):
            return None

    def _safe_bool(self, value: Optional[str]) -> bool:
        """Safely convert string to boolean"""
        if not value:
            return False
        value_lower = value.strip().lower()
        return value_lower in ('true', '1', 'yes', 't')

    def _calculate_url_quality_score(self, row: Dict[str, str]) -> int:
        """Calculate quality score for URL based on various factors"""
        score = 50  # Base score
        
        # PDF URL bonus (MOST IMPORTANT!)
        pdf_url = row.get('pdf_url', '').strip()
        if pdf_url:
            score += 25  # Big bonus for direct PDF access
        
        # Location type scoring
        location_type = row.get('location_type', '').lower()
        if location_type == 'best_oa':
            score += 20
        elif location_type == 'primary':
            score += 15
        elif location_type == 'alternate':
            score += 5
        
        # Open access scoring
        if self._safe_bool(row.get('is_oa', '')):
            score += 10
            
            oa_status = row.get('oa_status', '').lower()
            if oa_status == 'gold':
                score += 10
            elif oa_status == 'green':
                score += 5
        
        # Version scoring
        version = row.get('version', '').lower()
        if 'published' in version:
            score += 10
        elif 'accepted' in version:
            score += 5
        
        # Host type scoring
        host_type = row.get('host_type', '').lower()
        if host_type in ['journal', 'doaj_journal']:
            score += 10
        elif host_type == 'repository':
            score += 8
        elif host_type == 'preprint_server':
            score += 5
        
        # Domain reputation (simple heuristic)
        url = row.get('url', '').lower()
        reputable_domains = [
            'pubmed', 'pmc', 'doi.org', 'nature.com', 'science.org',
            'elsevier.com', 'springer.com', 'wiley.com', 'arxiv.org',
            'biorxiv.org', 'medrxiv.org'
        ]
        if any(domain in url for domain in reputable_domains):
            score += 5
        
        return min(100, max(0, score))  # Clamp between 0-100

    def read_csv_in_batches(self) -> Generator[List[Dict[str, Any]], None, None]:
        """Memory-efficient generator that yields batches of validated rows"""
        if not self.csv_file.exists():
            raise FileNotFoundError(f"CSV file not found: {self.csv_file}")

        logger.info(f"Reading CSV file: {self.csv_file}")

        # Check if file is empty or too small
        file_size = self.csv_file.stat().st_size
        if file_size == 0:
            raise ValueError("CSV file is empty")

        # Count lines to check if file has data
        with open(self.csv_file, 'r', encoding='utf-8') as f:
            line_count = sum(1 for _ in f)

        if line_count <= 1:
            raise ValueError(f"CSV file has no data rows (only {line_count} line(s) found)")

        logger.info(f"CSV file has {line_count:,} lines")

        # Use comma delimiter directly as specified
        delimiter = ','
        logger.info("Using comma as CSV delimiter")

        with open(self.csv_file, 'r', encoding='utf-8', errors='replace') as csvfile:
            try:
                reader = csv.DictReader(csvfile, delimiter=delimiter)

                # Validate CSV structure by checking first row
                try:
                    first_row = next(reader)
                    csvfile.seek(0)
                    reader = csv.DictReader(csvfile, delimiter=delimiter)  # Reset reader

                    # Check if we have expected columns
                    expected_columns = ['doi', 'url']  # Minimum required columns
                    missing_columns = [col for col in expected_columns if col not in first_row]
                    if missing_columns:
                        logger.warning(f"Missing expected columns: {missing_columns}")
                        logger.info(f"Available columns: {list(first_row.keys())}")
                        raise ValueError(f"CSV is missing required columns: {missing_columns}")

                    # Log the first row for debugging
                    logger.info(f"First row sample: {first_row}")

                except StopIteration:
                    raise ValueError("CSV file appears to be empty or has no data rows")

                # Initialize progress bar
                progress_bar = tqdm(
                    total=line_count - 1,  # Subtract 1 for header
                    desc="Processing CSV",
                    unit="rows",
                    unit_scale=True
                )

                current_batch = []
                batch_count = 0
                valid_rows = 0
                invalid_rows = 0

                try:
                    for row in reader:
                        cleaned_row = self.validate_and_clean_row(row)

                        if cleaned_row:
                            current_batch.append(cleaned_row)
                            valid_rows += 1
                        else:
                            invalid_rows += 1
                            self.stats['rows_skipped'] += 1

                        self.stats['total_rows_processed'] += 1
                        progress_bar.update(1)

                        # When batch is full, yield it and start a new one
                        if len(current_batch) >= self.batch_size:
                            batch_count += 1
                            logger.info(f"Yielding batch {batch_count} with {len(current_batch)} rows")
                            yield current_batch
                            current_batch = []

                    # Yield final batch if not empty
                    if current_batch:
                        batch_count += 1
                        logger.info(f"Yielding final batch {batch_count} with {len(current_batch)} rows")
                        yield current_batch

                finally:
                    progress_bar.close()
                    logger.info(f"CSV processing complete: {valid_rows} valid rows, {invalid_rows} invalid rows skipped")

            except Exception as e:
                logger.error(f"Error processing CSV: {e}")
                raise

    def _deduplicate_batch(self, batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        Remove duplicate (doi, url) pairs within a batch, keeping the record with highest quality score.

        Args:
            batch: List of validated row dictionaries

        Returns:
            Deduplicated list of row dictionaries
        """
        if not batch:
            return batch

        # Track duplicates using (doi, url) as key
        unique_records = {}
        duplicates_found = 0

        for row in batch:
            key = (row['doi'], row['url'])

            if key in unique_records:
                duplicates_found += 1
                existing_record = unique_records[key]

                # Smart deduplication: prefer records with PDF URLs, then higher quality scores
                existing_has_pdf = existing_record['pdf_url'] is not None and existing_record['pdf_url'].strip() != ''
                current_has_pdf = row['pdf_url'] is not None and row['pdf_url'].strip() != ''
                existing_score = existing_record['url_quality_score']
                current_score = row['url_quality_score']

                should_replace = False
                reason = ""

                if current_has_pdf and not existing_has_pdf:
                    # New record has PDF URL, existing doesn't - prefer new
                    should_replace = True
                    reason = f"new record has PDF URL (existing doesn't)"
                elif existing_has_pdf and not current_has_pdf:
                    # Existing has PDF URL, new doesn't - keep existing
                    should_replace = False
                    reason = f"existing record has PDF URL (new doesn't)"
                elif current_has_pdf and existing_has_pdf:
                    # Both have PDF URLs - merge and prefer higher quality score
                    should_replace = current_score > existing_score
                    reason = f"both have PDF URLs, quality scores: {current_score} vs {existing_score}"
                else:
                    # Neither has PDF URL - prefer higher quality score
                    should_replace = current_score > existing_score
                    reason = f"neither has PDF URL, quality scores: {current_score} vs {existing_score}"

                if should_replace:
                    # If we're replacing but the existing record had a PDF URL and new doesn't, preserve it
                    if existing_has_pdf and not current_has_pdf:
                        row['pdf_url'] = existing_record['pdf_url']
                        reason += " (preserved existing PDF URL)"

                    unique_records[key] = row
                    logger.debug(f"Replaced duplicate (doi={row['doi']}, url={row['url'][:50]}...) - {reason}")
                else:
                    # If we're keeping existing but new record has PDF URL and existing doesn't, update it
                    if current_has_pdf and not existing_has_pdf:
                        existing_record['pdf_url'] = row['pdf_url']
                        reason += " (updated with new PDF URL)"

                    logger.debug(f"Kept existing duplicate (doi={row['doi']}, url={row['url'][:50]}...) - {reason}")
            else:
                unique_records[key] = row

        if duplicates_found > 0:
            logger.debug(f"Deduplicated batch: removed {duplicates_found} duplicate (doi, url) pairs")
            self.stats['batch_duplicates'] += duplicates_found

        return list(unique_records.values())

    def insert_batch(self, batch: List[Dict[str, Any]], connection) -> Tuple[int, int]:
        """Insert a batch of rows using efficient bulk insert with conflict resolution"""

        if not batch:
            return 0, 0

        # Deduplicate the batch to prevent constraint violations
        deduplicated_batch = self._deduplicate_batch(batch)

        if len(deduplicated_batch) != len(batch):
            logger.debug(f"Batch size reduced from {len(batch)} to {len(deduplicated_batch)} after deduplication")

        # Prepare data for bulk insert
        insert_data = []
        for row in deduplicated_batch:
            # Ensure all required fields are present
            insert_data.append((
                row['doi'],
                row['url'],
                row.get('pdf_url'),
                row.get('openalex_id'),
                row.get('title'),
                row.get('publication_year'),
                row.get('location_type', 'unknown'),  # Provide default for required field
                row.get('version'),
                row.get('license'),
                row.get('host_type'),
                row.get('oa_status'),
                row.get('is_oa', False),
                row.get('url_quality_score', 50)
            ))
        
        rows_inserted = 0
        rows_updated = 0
        
        try:
            with connection.cursor() as cur:
                # Get count before insert
                cur.execute("SELECT COUNT(*) FROM doi_urls")
                count_before = cur.fetchone()[0]
                
                # Use a very small chunk size for reliability
                chunk_size = 50
                
                for i in range(0, len(insert_data), chunk_size):
                    chunk = insert_data[i:i+chunk_size]
                    logger.info(f"Processing chunk {i//chunk_size + 1}/{(len(insert_data) + chunk_size - 1) // chunk_size} ({len(chunk)} rows)")
                    
                    # Try row-by-row insertion for maximum reliability
                    for j, row_data in enumerate(chunk):
                        try:
                            cur.execute("""
                                INSERT INTO doi_urls (
                                    doi, url, pdf_url, openalex_id, title, publication_year, location_type,
                                    version, license, host_type, oa_status, is_oa, url_quality_score
                                ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                                ON CONFLICT (doi, url) DO UPDATE SET
                                    pdf_url = CASE
                                        WHEN EXCLUDED.pdf_url IS NOT NULL AND EXCLUDED.pdf_url != ''
                                        THEN EXCLUDED.pdf_url
                                        ELSE doi_urls.pdf_url
                                    END,
                                    openalex_id = EXCLUDED.openalex_id,
                                    title = EXCLUDED.title,
                                    publication_year = EXCLUDED.publication_year,
                                    location_type = EXCLUDED.location_type,
                                    version = EXCLUDED.version,
                                    license = EXCLUDED.license,
                                    host_type = EXCLUDED.host_type,
                                    oa_status = EXCLUDED.oa_status,
                                    is_oa = EXCLUDED.is_oa,
                                    url_quality_score = EXCLUDED.url_quality_score,
                                    updated_at = CURRENT_TIMESTAMP
                                RETURNING (xmax = 0) AS inserted
                            """, row_data)
                            
                            # Check if row was inserted or updated
                            result = cur.fetchone()
                            if result and result[0]:  # xmax = 0 means inserted
                                rows_inserted += 1
                            else:
                                rows_updated += 1
                            
                            # Commit every 10 rows
                            if (j + 1) % 10 == 0:
                                connection.commit()
                                
                        except Exception as e:
                            connection.rollback()
                            logger.error(f"Error inserting row {i+j}: {e}")
                            # Continue with next row
                
                # Commit any remaining rows in this chunk
                connection.commit()
                logger.info(f"Committed chunk {i//chunk_size + 1}: {rows_inserted} inserted, {rows_updated} updated so far")
            
                # Get count after insert  
                cur.execute("SELECT COUNT(*) FROM doi_urls")
                count_after = cur.fetchone()[0]
                
                # Verify counts
                actual_diff = count_after - count_before
                logger.info(f"Database count change: +{actual_diff} rows (expected +{rows_inserted})")
                
                return rows_inserted, rows_updated
                
        except psycopg2.Error as e:
            connection.rollback()
            logger.error(f"Batch insert failed: {e}")
            raise

    def update_doi_metadata(self, connection):
        """Update the separate doi_metadata table with aggregated information"""
        logger.info("Updating DOI metadata table...")
        
        metadata_sql = """
        INSERT INTO doi_metadata (doi, openalex_id, title, publication_year)
        SELECT DISTINCT 
            doi,
            openalex_id,
            title,
            publication_year
        FROM doi_urls
        WHERE doi IS NOT NULL
        ON CONFLICT (doi) DO UPDATE SET
            openalex_id = EXCLUDED.openalex_id,
            title = EXCLUDED.title,
            publication_year = EXCLUDED.publication_year,
            updated_at = CURRENT_TIMESTAMP;
        """
        
        try:
            with connection.cursor() as cur:
                cur.execute(metadata_sql)
                rows_affected = cur.rowcount
                connection.commit()
                logger.debug(f"Updated {rows_affected} DOI metadata records")
        except psycopg2.Error as e:
            connection.rollback()
            logger.error(f"DOI metadata update failed: {e}")

    def print_final_stats(self):
        """Print import statistics"""
        duration = self.stats['end_time'] - self.stats['start_time']

        print("\n" + "=" * 60)
        print("DOI-URL IMPORT COMPLETED")
        print("=" * 60)
        print(f"Total rows processed: {self.stats['total_rows_processed']:,}")
        print(f"Rows inserted: {self.stats['rows_inserted']:,}")
        print(f"Rows updated: {self.stats['rows_updated']:,}")
        print(f"Rows skipped (invalid): {self.stats['rows_skipped']:,}")
        print(f"Batch duplicates removed: {self.stats['batch_duplicates']:,}")
        print(f"Invalid URLs filtered: {self.stats['invalid_urls']:,}")
        print(f"Import duration: {duration:.1f} seconds")
        if duration > 0:
            print(f"Processing rate: {self.stats['total_rows_processed']/duration:.1f} rows/sec")
        print("=" * 60)

    def run_import(self):
        """Execute the complete import process"""
        logger.info("Starting DOI-URL import process...")
        self.stats['start_time'] = time.time()

        try:
            # Create schema if requested
            if self.create_tables:
                self.create_schema()

            # Test database connection and insertion
            test_success = self.test_database_connection()
            if not test_success:
                logger.error("Database test failed - cannot proceed with import")
                return

            # Process CSV in batches using memory-efficient generator
            logger.info("Reading CSV file and processing batches...")
            batch_generator = self.read_csv_in_batches()
            
            # Process batches directly without converting to list
            with self.connect_db() as conn:
                # Get initial count
                with conn.cursor() as cur:
                    cur.execute("SELECT COUNT(*) FROM doi_urls")
                    initial_count = cur.fetchone()[0]
                    logger.info(f"Initial database count: {initial_count} rows")
                
                batch_count = 0
                total_rows_processed = 0
                
                # Process each batch as it comes from the generator
                for batch in batch_generator:
                    batch_count += 1
                    total_rows_processed += len(batch)
                    
                    logger.info(f"Processing batch {batch_count} with {len(batch)} rows (total processed: {total_rows_processed})")
                    
                    # Try inserting the first row of each batch as a test
                    if batch and batch_count % 10 == 1:  # Test every 10th batch
                        test_row = batch[0]
                        logger.info(f"Testing single row insert for batch {batch_count}: DOI={test_row['doi']}")
                        row_success = self.insert_single_row(test_row, conn)
                        if not row_success:
                            logger.error(f"Failed to insert test row from batch {batch_count}")
                    
                    # Now try the full batch
                    try:
                        # Get count before batch
                        with conn.cursor() as cur:
                            cur.execute("SELECT COUNT(*) FROM doi_urls")
                            before_count = cur.fetchone()[0]
                        
                        # Insert batch
                        rows_inserted, rows_updated = self.insert_batch(batch, conn)
                        
                        # Get count after batch
                        with conn.cursor() as cur:
                            cur.execute("SELECT COUNT(*) FROM doi_urls")
                            after_count = cur.fetchone()[0]
                        
                        # Verify counts match expectations
                        actual_diff = after_count - before_count
                        expected_diff = rows_inserted
                        
                        if actual_diff != expected_diff:
                            logger.warning(f"Count mismatch: Expected +{expected_diff} rows, got +{actual_diff} rows")
                            logger.warning(f"Before: {before_count}, After: {after_count}, Reported inserted: {rows_inserted}")
                        
                        self.stats['rows_inserted'] += rows_inserted
                        self.stats['rows_updated'] += rows_updated
                        
                        logger.info(f"Batch {batch_count} result: +{actual_diff} rows in database, {rows_inserted} reported inserted, {rows_updated} updated")
                        
                        # If no rows were inserted despite having data, try direct insertion
                        if actual_diff == 0 and len(batch) > 0 and rows_inserted > 0:
                            logger.warning(f"Batch {batch_count}: No rows added to database despite successful operation")
                            logger.info("Trying direct row-by-row insertion as fallback")
                            
                            # Try inserting a few rows directly
                            for i, row in enumerate(batch[:5]):  # Try first 5 rows
                                success = self.insert_single_row(row, conn)
                                logger.info(f"Direct insert of row {i}: {'Success' if success else 'Failed'}")
                    
                    except Exception as e:
                        logger.error(f"Error processing batch {batch_count}: {e}")
                        # Continue with next batch
                    
                    # Check database count periodically
                    if batch_count % 5 == 0:
                        with conn.cursor() as cur:
                            cur.execute("SELECT COUNT(*) FROM doi_urls")
                            current_count = cur.fetchone()[0]
                            logger.info(f"Current database count: {current_count} rows (+{current_count - initial_count} from start)")
                    
                    # Optional: Stop after a few batches in debug mode to check what's happening
                    # Commented out to allow full import even in debug mode
                    # if batch_count >= 5 and logger.level == logging.DEBUG:
                    #     logger.debug("Debug mode: stopping after 5 batches for analysis")
                    #     break
                
                # Get final count
                with conn.cursor() as cur:
                    cur.execute("SELECT COUNT(*) FROM doi_urls")
                    final_count = cur.fetchone()[0]
                    logger.info(f"Final database count: {final_count} rows (+{final_count - initial_count} from start)")

            self.stats['end_time'] = time.time()
            self.print_final_stats()

        except Exception as e:
            logger.error(f"Import failed: {e}")
            raise

    def test_database_connection(self):
        """Test database connection and insertion capability"""
        logger.info("Testing database connection and insertion...")
        
        test_data = {
            'doi': '10.1234/test',
            'url': 'https://example.com/test',
            'pdf_url': 'https://example.com/test.pdf',
            'location_type': 'test',
            'is_oa': True,
            'url_quality_score': 75
        }
        
        try:
            with self.connect_db() as conn:
                # Check if we can connect
                logger.info("Database connection successful")
                
                # Check database version
                with conn.cursor() as cur:
                    cur.execute("SELECT version()")
                    version = cur.fetchone()[0]
                    logger.info(f"PostgreSQL version: {version}")
                
                # Check if table exists
                with conn.cursor() as cur:
                    cur.execute("SELECT EXISTS (SELECT FROM information_schema.tables WHERE table_name = 'doi_urls')")
                    table_exists = cur.fetchone()[0]
                    logger.info(f"doi_urls table exists: {table_exists}")

                    if table_exists:
                        # Check table structure
                        self.check_database_constraints(conn)

                # Test insertion
                try:
                    with conn.cursor() as cur:
                        cur.execute("""
                            INSERT INTO doi_urls (doi, url, pdf_url, location_type, is_oa, url_quality_score)
                            VALUES (%s, %s, %s, %s, %s, %s)
                            ON CONFLICT (doi, url) DO UPDATE SET
                                updated_at = CURRENT_TIMESTAMP
                            RETURNING id
                        """, (
                            test_data['doi'],
                            test_data['url'],
                            test_data['pdf_url'],
                            test_data['location_type'],
                            test_data['is_oa'],
                            test_data['url_quality_score']
                        ))

                        result = cur.fetchone()
                        conn.commit()

                        logger.info(f"Test insertion successful, row id: {result[0]}")

                        # Verify the row was actually inserted
                        cur.execute("SELECT COUNT(*) FROM doi_urls WHERE doi = %s AND url = %s",
                                   (test_data['doi'], test_data['url']))
                        count = cur.fetchone()[0]
                        logger.info(f"Verification query found {count} matching rows")

                        return count > 0

                except Exception as e:
                    conn.rollback()
                    logger.error(f"Database test failed: {e}")
                    return False
                
        except Exception as e:
            logger.error(f"Database connection failed: {e}")
            return False

    def insert_single_row(self, row: Dict[str, Any], connection) -> bool:
        """Insert a single row for testing purposes"""
        logger.info(f"Inserting single test row: {row['doi']}")
        
        try:
            with connection.cursor() as cur:
                cur.execute("""
                    INSERT INTO doi_urls (
                        doi, url, pdf_url, openalex_id, title, publication_year, location_type,
                        version, license, host_type, oa_status, is_oa, url_quality_score
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                    ON CONFLICT (doi, url) DO UPDATE SET
                        updated_at = CURRENT_TIMESTAMP
                    RETURNING id
                """, (
                    row['doi'],
                    row['url'],
                    row.get('pdf_url'),
                    row.get('openalex_id'),
                    row.get('title'),
                    row.get('publication_year'),
                    row.get('location_type', 'unknown'),
                    row.get('version'),
                    row.get('license'),
                    row.get('host_type'),
                    row.get('oa_status'),
                    row.get('is_oa', False),
                    row.get('url_quality_score', 50)
                ))
                
                result = cur.fetchone()
                connection.commit()
                
                logger.info(f"Single row insert result: {result}")
                return True
                
        except Exception as e:
            connection.rollback()
            logger.error(f"Single row insert failed: {e}")
            return False

    def check_database_constraints(self, connection):
        """Check database constraints that might be preventing inserts"""
        logger.info("Checking database constraints...")
        
        try:
            with connection.cursor(cursor_factory=RealDictCursor) as cur:
                # Check table definition
                cur.execute("""
                    SELECT column_name, data_type, is_nullable
                    FROM information_schema.columns
                    WHERE table_name = 'doi_urls'
                    ORDER BY ordinal_position
                """)
                columns = cur.fetchall()
                logger.info("Table structure:")
                for col in columns:
                    logger.info(f"  {col['column_name']}: {col['data_type']} (nullable: {col['is_nullable']})")
                
                # Check constraints
                cur.execute("""
                    SELECT conname, contype, pg_get_constraintdef(oid) as def
                    FROM pg_constraint
                    WHERE conrelid = 'doi_urls'::regclass
                """)
                constraints = cur.fetchall()
                logger.info("Table constraints:")
                for con in constraints:
                    logger.info(f"  {con['conname']} ({con['contype']}): {con['def']}")
                
                # Check indexes
                cur.execute("""
                    SELECT indexname, indexdef
                    FROM pg_indexes
                    WHERE tablename = 'doi_urls'
                """)
                indexes = cur.fetchall()
                logger.info("Table indexes:")
                for idx in indexes:
                    logger.info(f"  {idx['indexname']}: {idx['indexdef']}")
                
                # Check for triggers
                cur.execute("""
                    SELECT trigger_name, action_statement
                    FROM information_schema.triggers
                    WHERE event_object_table = 'doi_urls'
                """)
                triggers = cur.fetchall()
                logger.info("Table triggers:")
                for trig in triggers:
                    logger.info(f"  {trig['trigger_name']}: {trig['action_statement']}")
                
                return True
                
        except Exception as e:
            logger.error(f"Error checking database constraints: {e}")
            return False


def main():
    parser = argparse.ArgumentParser(description='Import DOI-URL CSV into PostgreSQL')
    parser.add_argument('--csv-file', required=True,
                        help='Path to CSV file with DOI-URL mappings')
    parser.add_argument('--db-host', default='localhost',
                        help='PostgreSQL host')
    parser.add_argument('--db-port', type=int, default=5432,
                        help='PostgreSQL port')
    parser.add_argument('--db-name', required=True,
                        help='PostgreSQL database name')
    parser.add_argument('--db-user', required=True,
                        help='PostgreSQL username')
    parser.add_argument('--db-password', required=True,
                        help='PostgreSQL password')
    parser.add_argument('--batch-size', type=int, default=10000,
                        help='Batch size for bulk inserts (default: 10000)')
    parser.add_argument('--skip-create-tables', action='store_true',
                        help='Skip table creation (tables already exist)')
    parser.add_argument('--test-only', action='store_true',
                        help='Only test database connection and exit')
    parser.add_argument('--debug', action='store_true',
                        help='Enable debug logging')
    
    args = parser.parse_args()
    
    # Set debug logging if requested
    if args.debug:
        logger.setLevel(logging.DEBUG)
        for handler in logger.handlers:
            handler.setLevel(logging.DEBUG)
    
    # Database configuration
    db_config = {
        'host': args.db_host,
        'port': args.db_port,
        'database': args.db_name,
        'user': args.db_user,
        'password': args.db_password
    }
    
    # Create importer
    importer = DOIURLImporter(
        db_config=db_config,
        csv_file=args.csv_file,
        batch_size=args.batch_size,
        create_tables=not args.skip_create_tables
    )
    
    # Test database connection
    if args.test_only or args.debug:
        success = importer.test_database_connection()
        if args.test_only:
            sys.exit(0 if success else 1)
    
    # Run the import
    importer.run_import()


if __name__ == '__main__':
    main()
